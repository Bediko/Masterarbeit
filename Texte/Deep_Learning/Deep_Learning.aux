\relax 
\providecommand{\transparent@use}[1]{}
\citation{bachelor}
\@writefile{toc}{\contentsline {section}{\numberline {1}Einleitung}{2}}
\citation{PDP}
\@writefile{toc}{\contentsline {section}{\numberline {2}Grundlagen}{3}}
\newlabel{Grundlagen}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}K\IeC {\"u}nstliche neuronale Netze}{3}}
\newlabel{NN}{{2.1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Neuron (Quelle: http://commons.wikimedia.org/wiki/File:Neuron\_-\_annotated.svg) \relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Neuron}{{1}{3}}
\citation{PDP}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Zwei miteinander verbundene Neuronen in einem Neuronalen Netz\relax }}{4}}
\newlabel{2n}{{2}{4}}
\citation{PDP}
\citation{PDP}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Backpropagation-Algorithmus}{5}}
\newlabel{BP}{{2.2}{5}}
\newlabel{fehler}{{6}{5}}
\citation{PDP}
\newlabel{anpassung}{{7}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Neuronales Netz. Die Ein- und Ausgabeebene verwendet eine lineare Aktivierungsfunktion, die versteckte Ebene den Tangens Hyperbolicus.\relax }}{7}}
\newlabel{MLPBeispiel}{{3}{7}}
\newlabel{OANN}{{16}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Markov-Ketten}{9}}
\citation{Hopfield}
\citation{BM}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \IeC {\"U}bergangsgraph. Knoten geben m\IeC {\"o}gliche Zust\IeC {\"a}nde an, Kanten sind Wahrscheinlichkeiten in einen Zustand zu wechseln\relax }}{10}}
\newlabel{Markovc}{{4}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Hopfield Netze}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Modell eines Hopfield Netzes. Alle Neuronen besitzen eine Ein- und Ausgabe und sind \IeC {\"u}ber Gewichte jeweils mit allen anderen Neuronen verbunden\relax }}{10}}
\newlabel{HopfieldNetz}{{5}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Boltzmann Maschinen}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Modell einer Boltzmann Maschine. Gelbe Kreise stellen verstecke Neuronen dar, wei\IeC {\ss }e Kreise sind sichtbare Neuronen.\relax }}{11}}
\newlabel{Boltzmannmaschine}{{6}{11}}
\citation{KLD}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Eingeschr\IeC {\"a}nkte Boltzmann Maschinen}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Modell einer eingeschr\IeC {\"a}nkten Boltzmann Maschine. Oben sind die versteckten Knoten und unten die nach au\IeC {\ss }en sichtbaren Knoten.\relax }}{13}}
\newlabel{RBM}{{7}{13}}
\citation{guide}
\citation{guide}
\newlabel{ph}{{41}{15}}
\newlabel{pv}{{42}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Markov-Kette mit Gibbs Sampling. Diese wird mit einem Trainingsvektor initialisiert. Die sichtbaren und unsichtbaren Neuronen werden abwechselnd neu berechnet bis ein Equilibriumszustand erreicht wird.\relax }}{15}}
\newlabel{Markov}{{8}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Kontrastive Divergenz}{15}}
\citation{KLD}
\citation{noconv}
\citation{digits}
\@writefile{toc}{\contentsline {section}{\numberline {3}Deep-Belief Netze}{16}}
\citation{learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Greedy Algorithmus zum Trainieren von Deep-Belief Netzen}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Hyprid Netzwerk. Die Ebenen $H_3$ und $H_2$ sind mit ungerichteten Kanten verbunden und bilden einen Assoziativspeicher. Die anderen Ebenen sind mit gerichteten Kanten verbunden.\relax }}{17}}
\newlabel{Netz}{{9}{17}}
\citation{learning}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Beispiel f\IeC {\"u}r den Explained Away Effekt. Der Bias von $-10$ am Erdbeben-knoten bedeutet dass dieser ohne beobachtet zu werden $e^{10}$ mal wahrscheinlicher aus als aktiviert ist. Beide Ereignisse zusammen haben eine Wahrscheinlichkeit von $e^{-20}$, da diese unabh\IeC {\"a}ngig voneinander sind. Dies ist also sehr unwahrscheinlich. Deshalb erkl\IeC {\"a}rt die Aktivierung eines Knotens den anderen weg.\relax }}{18}}
\newlabel{ExplainedAway}{{10}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Deep-Belief Netze zur Klassifikation}{19}}
\citation{backprop}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Netzwerk mit Softmax-Gruppe zum Klassifizieren. Aus Sicht von $H_3$ werden die Softmax-Neuronen als normale Eingabe betrachtet. An der Softmax-Gruppe kann sp\IeC {\"a}ter die gefundene Klasse abgelesen werden.\relax }}{20}}
\newlabel{SMTrain}{{11}{20}}
\citation{Helm}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Netzwerk mit Ausgabeschicht zur Klassifizierung. Die Gewichte zwischen $H_3$ und der Ausgabe werden mit Backpropagation trainiert.\relax }}{21}}
\newlabel{BPTrain}{{12}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation}{21}}
\citation{Helm}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Allgemeiner Aufbau des Frameworks}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Klassenherarchie des verwendeten Frameworks, rot umrandete Klassen wurden in dieser Arbeit hinzugef\IeC {\"u}gt.\relax }}{22}}
\newlabel{Aufbau}{{13}{22}}
\newlabel{NNKonf}{{1}{22}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Beispielkonfigurationsdatei f\IeC {\"u}r ein neuronales Netz}{22}}
\newlabel{DatenConf}{{2}{23}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}Auszug aus einem der verwendeten Datens\IeC {\"a}tze}{23}}
\citation{Helm}
\citation{Helm}
\newlabel{BPConf}{{3}{24}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3}Beispielkonfiguration f\IeC {\"u}r den Backpropagation-Algorithmus}{24}}
\newlabel{CDConf}{{4}{24}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4}Beispielkonfiguration der Kontrastiven Divergenz}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Erweiterungen des Frameworks}{25}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Implementation der Kontrastiven Divergenz\relax }}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Versuch}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Achsensymmetrie und Rotationssymmetrie}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}MNIST Datenbank}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Versuchsdurchf\IeC {\"u}hrung}{27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Achsensymetrie und Rotationssymetrie}{27}}
\citation{learning}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Vergleich verschiedener Netztopologien mit einem Ausgabeneuron\relax }}{28}}
\newlabel{sigmoid}{{1}{28}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Vergleich verschiedener Netztopologien mit Softmax-Neuronen als Ausgabe\relax }}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}MNIST Datenbank}{28}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Vergleich verschiedener Netztopologien mit Softmax-Neuronen als Ausgabe\relax }}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Probleme von Deep-Belief Netzen}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Einfache Verbesserung}{29}}
\bibstyle{unsrt}
\bibcite{BM}{1}
\bibcite{guide}{2}
\bibcite{digits}{3}
\bibcite{noconv}{4}
\bibcite{learning}{5}
\bibcite{backprop}{6}
\bibcite{Hopfield}{7}
\bibcite{KLD}{8}
\bibcite{PDP}{9}
\bibcite{bachelor}{10}
\bibcite{Helm}{11}
\bibcite{Markovchains}{12}
