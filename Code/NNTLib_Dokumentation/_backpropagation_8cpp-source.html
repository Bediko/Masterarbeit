<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1">
<title>NeuralNetworkTrainer: Backpropagation.cpp Quellcode</title>
<link href="doxygen.css" rel="stylesheet" type="text/css">
<link href="tabs.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Erzeugt von Doxygen 1.4.5 -->
<div class="tabs">
  <ul>
    <li><a href="main.html"><span>Hauptseite</span></a></li>
    <li><a href="annotated.html"><span>Klassen</span></a></li>
    <li id="current"><a href="files.html"><span>Dateien</span></a></li>
    <li><a href="dirs.html"><span>Verzeichnisse</span></a></li>
  </ul></div>
<div class="nav">
<a class="el" href="dir_E_3A_2F.html">E:</a>&nbsp;&raquo&nbsp;<a class="el" href="dir_E_3A_2FHochschule_20Niederrhein_2F.html">Hochschule Niederrhein</a>&nbsp;&raquo&nbsp;<a class="el" href="dir_E_3A_2FHochschule_20Niederrhein_2FSemester6_2F.html">Semester6</a>&nbsp;&raquo&nbsp;<a class="el" href="dir_E_3A_2FHochschule_20Niederrhein_2FSemester6_2FNN_2F.html">NN</a>&nbsp;&raquo&nbsp;<a class="el" href="dir_E_3A_2FHochschule_20Niederrhein_2FSemester6_2FNN_2FC_5FIMP_2F.html">C_IMP</a>&nbsp;&raquo&nbsp;<a class="el" href="dir_E_3A_2FHochschule_20Niederrhein_2FSemester6_2FNN_2FC_5FIMP_2FNeuralNetworkTrainer_2F.html">NeuralNetworkTrainer</a>&nbsp;&raquo&nbsp;<a class="el" href="dir_E_3A_2FHochschule_20Niederrhein_2FSemester6_2FNN_2FC_5FIMP_2FNeuralNetworkTrainer_2FNeuralNetworkTrainer_2F.html">NeuralNetworkTrainer</a></div>
<h1>Backpropagation.cpp</h1><div class="fragment"><pre class="fragment"><a name="l00001"></a>00001 <span class="preprocessor">#include "Backpropagation.h"</span>
<a name="l00002"></a>00002 <span class="preprocessor">#include "Helper.h"</span>
<a name="l00003"></a>00003 
<a name="l00004"></a><a class="code" href="class_backpropagation.html#b280cdda3f671e544ae9e26a027bb319">00004</a> <a class="code" href="class_backpropagation.html#b280cdda3f671e544ae9e26a027bb319">Backpropagation::Backpropagation</a>()
<a name="l00005"></a>00005 {
<a name="l00006"></a>00006 }
<a name="l00007"></a>00007 
<a name="l00008"></a><a class="code" href="class_backpropagation.html#2bb0f170484f1b63863c72efaa7ee62c">00008</a> <a class="code" href="class_backpropagation.html#2bb0f170484f1b63863c72efaa7ee62c">Backpropagation::~Backpropagation</a>()
<a name="l00009"></a>00009 {
<a name="l00010"></a>00010 }
<a name="l00011"></a>00011 
<a name="l00012"></a>00012 
<a name="l00013"></a>00013 <span class="comment">//NeuralNetwork Backpropagation::TrainBatch(BackpropagationConfig config,DataContainer * container)</span>
<a name="l00014"></a>00014 <span class="comment">//{</span>
<a name="l00015"></a>00015 <span class="comment">//      NeuralNetwork net;</span>
<a name="l00016"></a>00016 <span class="comment">//      return net;</span>
<a name="l00017"></a>00017 <span class="comment">//}</span>
<a name="l00018"></a>00018 
<a name="l00019"></a>00019 
<a name="l00026"></a><a class="code" href="class_backpropagation.html#cce34b65dfa3dba408c47a7860ccf74a">00026</a> <span class="keywordtype">void</span> <a class="code" href="class_backpropagation.html#cce34b65dfa3dba408c47a7860ccf74a">Backpropagation::Train</a>(<a class="code" href="class_neural_network.html">NeuralNetwork</a> *net, <a class="code" href="struct_data_container.html">DataContainer</a> *container,<a class="code" href="class_backpropagation_config.html">BackpropagationConfig</a> config)
<a name="l00027"></a>00027 {
<a name="l00028"></a>00028         <a class="code" href="class_backpropagation.html#cce34b65dfa3dba408c47a7860ccf74a">Train</a>(net,container,config.<a class="code" href="class_backpropagation_config.html#6132295fcf5570fb8b0a944ef322a598">Alpha</a>,config.<a class="code" href="class_backpropagation_config.html#37323780191d51a33d295ac10fb98f06">Momentum</a>,config.<a class="code" href="class_backpropagation_config.html#0099bc3485a0dd068896f9e0aaa198b0">MaxLoopCount</a>,config.<a class="code" href="class_backpropagation_config.html#506889eecd215a6701e67840b662403f">BatchSize</a>,config.<a class="code" href="class_backpropagation_config.html#96121cb56d34d7e88003f432c3163a31">ErrorThreshold</a>);
<a name="l00029"></a>00029 }
<a name="l00030"></a>00030 
<a name="l00031"></a><a class="code" href="class_backpropagation.html#d6ba317c113dddbb5c5d3d2af7f89c4c">00031</a> <span class="keywordtype">void</span> <a class="code" href="class_backpropagation.html#cce34b65dfa3dba408c47a7860ccf74a">Backpropagation::Train</a>(<a class="code" href="class_neural_network.html">NeuralNetwork</a>* net, <a class="code" href="struct_data_container.html">DataContainer</a> * container,<span class="keyword">const</span> <span class="keywordtype">double</span> alpha,<span class="keyword">const</span> <span class="keywordtype">double</span> momentum,<span class="keyword">const</span> <span class="keywordtype">int</span> loopcount,<span class="keywordtype">int</span> batchSize,<span class="keywordtype">double</span> threshold)
<a name="l00032"></a>00032 {
<a name="l00033"></a>00033         <span class="keywordflow">if</span>(batchSize &gt; container-&gt;<a class="code" href="struct_data_container.html#d8e19b2c95728183a73f405df4be2c6a">DataCount</a>)
<a name="l00034"></a>00034                 batchSize = container-&gt;<a class="code" href="struct_data_container.html#d8e19b2c95728183a73f405df4be2c6a">DataCount</a>;<span class="comment">//vermeiden das bei zu gro√üer batchsize einfach garnichts passiert</span>
<a name="l00035"></a>00035 
<a name="l00036"></a>00036         <span class="comment">//double * backPropagationResults = new double[loopcount]();</span>
<a name="l00037"></a>00037         <span class="comment">//net-&gt;MSEIterationResults = backPropagationResults;</span>
<a name="l00038"></a>00038         <span class="comment">//net-&gt;MSEIterationResultsLenght = loopcount;</span>
<a name="l00039"></a>00039 
<a name="l00040"></a>00040         <a class="code" href="struct_neural_network_measure.html">NeuralNetworkMeasure</a> * measureResult = <span class="keyword">new</span> <a class="code" href="struct_neural_network_measure.html">NeuralNetworkMeasure</a>[loopcount]();
<a name="l00041"></a>00041 
<a name="l00042"></a>00042         <span class="comment">//lokale variablen zur verbesserung der performance</span>
<a name="l00043"></a>00043         <span class="comment">//Zugriffszeit aus weights[k] besser als auf net-&gt;Layers[i]-&gt;layer-&gt;Neurons[j]-&gt;weights[k];</span>
<a name="l00044"></a>00044         <span class="keywordtype">int</span> i,j,k,l;
<a name="l00045"></a>00045         <span class="keywordtype">double</span> error;
<a name="l00046"></a>00046         <span class="keywordtype">double</span> *deltaWeights;
<a name="l00047"></a>00047         <a class="code" href="class_layer.html">Layer</a>* layer;
<a name="l00048"></a>00048         <a class="code" href="class_neuron.html">Neuron</a> *neuron;
<a name="l00049"></a>00049         <span class="keywordtype">double</span> *weights;
<a name="l00050"></a>00050         <span class="keywordtype">double</span> *lastDeltaWeights;
<a name="l00051"></a>00051         <span class="keywordtype">double</span> *inputVector;
<a name="l00052"></a>00052         <span class="keywordtype">double</span> *sumDeltaErrWeights = NULL;
<a name="l00053"></a>00053         <span class="keywordtype">double</span> *sumDeltaErrWeightsNext;
<a name="l00054"></a>00054         <span class="keywordtype">double</span> error_mul_alpha;
<a name="l00055"></a>00055         <span class="keywordtype">double</span> deltaWeight;
<a name="l00056"></a>00056         <span class="keywordtype">int</span> inputVectorCount;
<a name="l00057"></a>00057 
<a name="l00058"></a>00058         <span class="keywordtype">unsigned</span> <span class="keywordtype">long</span> <span class="keywordtype">long</span> time64= GetTimeMs64();
<a name="l00059"></a>00059 
<a name="l00060"></a>00060         <span class="keywordflow">for</span>(l=0;l&lt;loopcount;++l)
<a name="l00061"></a>00061         {
<a name="l00062"></a>00062                 <span class="keywordflow">for</span>(<span class="keywordtype">int</span> c=0;c&lt;container-&gt;DataCount;++c)
<a name="l00063"></a>00063                 {
<a name="l00064"></a>00064                         <span class="comment">//Daten durch das netz "propagieren"</span>
<a name="l00065"></a>00065                         net-&gt;<a class="code" href="class_neural_network.html#25fd0ec0e96825cc905ad2a2dca8318d">Propagate</a>(container-&gt;<a class="code" href="struct_data_container.html#81fb025d589a4404e3053c4f0f66a11d">DataInput</a>[c]);
<a name="l00066"></a>00066                         <span class="comment">//continue;</span>
<a name="l00067"></a>00067                         <span class="comment">//Fehler des letzten Layers berechnen, Gewichte Anpassen und Gewichtetete Fehler f√ºr Layer l-1 berechnen</span>
<a name="l00068"></a>00068 
<a name="l00069"></a>00069                         <span class="comment">//pointer zugriffe zu minimieren</span>
<a name="l00070"></a>00070                         <span class="keywordtype">int</span> lastLayerIndex = net-&gt;<a class="code" href="class_neural_network.html#7336a29c2bf28b28f785675641df2d44">LayersCount</a> - 1;
<a name="l00071"></a>00071 
<a name="l00072"></a>00072                         <span class="keywordflow">for</span>(i=lastLayerIndex;i&gt;=1;i--)<span class="comment">//zur√ºck propagieren fangen beim letzten Layer an</span>
<a name="l00073"></a>00073                         {
<a name="l00074"></a>00074                                 layer = &amp;net-&gt;<a class="code" href="class_neural_network.html#87bfda183c4f851a101e97bbb1bbace7">Layers</a>[i];
<a name="l00075"></a>00075                                 inputVector = layer-&gt;<a class="code" href="class_layer.html#520efaff1a72c027956083ff9656d725">InputVector</a>;
<a name="l00076"></a>00076                                 sumDeltaErrWeightsNext = layer-&gt;<a class="code" href="class_layer.html#31c459a9478b4716269826458168ec74">SumDeltaErrWeights</a>;
<a name="l00077"></a>00077                                 inputVectorCount = layer-&gt;<a class="code" href="class_layer.html#74697d5bce4cac5ae28f7d2aa266d9d8">InputVectorCount</a>;
<a name="l00078"></a>00078 
<a name="l00079"></a>00079                                 <span class="keywordflow">for</span>(j=0;j&lt;layer-&gt;NeuronCount;++j)
<a name="l00080"></a>00080                                 {
<a name="l00081"></a>00081                                         neuron = &amp;layer-&gt;<a class="code" href="class_layer.html#2d092d92942366158bc7e1fa013a74dc">Neurons</a>[j];
<a name="l00082"></a>00082                                         weights = neuron-&gt;<a class="code" href="class_neuron.html#8831731b52fdc6cb7f162f9767d66e4d">Weights</a>;
<a name="l00083"></a>00083                                         lastDeltaWeights = neuron-&gt;<a class="code" href="class_neuron.html#a4b1e68a202cbb6e927caa848f83ab49">LastDeltaWeights</a>;
<a name="l00084"></a>00084                                         deltaWeights = neuron-&gt;<a class="code" href="class_neuron.html#6b4cb6ab049e62a2ed019c9f64e3834b">DeltaWeights</a>;
<a name="l00085"></a>00085 
<a name="l00086"></a>00086                                         <span class="keywordflow">if</span>(i==lastLayerIndex)<span class="comment">//Letzter Layer</span>
<a name="l00087"></a>00087                                         {
<a name="l00088"></a>00088                                                 <span class="comment">//Fehler berechnen , error = f_act'(net) * (t_i - o_j)</span>
<a name="l00089"></a>00089                                                 error=  ActivationFunctionDerivate(net-&gt;<a class="code" href="class_neural_network.html#929f1f7a270c41cfd9e987b88e048aaf">FunctionType</a>,neuron-&gt;<a class="code" href="class_neuron.html#29c2c02a361c9d7028472e5d92cd4a54">Output</a>) * (container-&gt;<a class="code" href="struct_data_container.html#e52a283505cf5b64b4c50c462609fa24">DataDesiredOutput</a>[c][j] - neuron-&gt;<a class="code" href="class_neuron.html#29c2c02a361c9d7028472e5d92cd4a54">Output</a>);
<a name="l00090"></a>00090 
<a name="l00091"></a>00091                                                 <span class="comment">//Mittleren Quadratischen Fehler berechnen</span>
<a name="l00092"></a>00092                                                 net-&gt;<a class="code" href="class_neural_network.html#5597b68c1d11a9438bb2f0d54623ce75">MeanSquareError</a>+=(container-&gt;<a class="code" href="struct_data_container.html#e52a283505cf5b64b4c50c462609fa24">DataDesiredOutput</a>[c][j] - neuron-&gt;<a class="code" href="class_neuron.html#29c2c02a361c9d7028472e5d92cd4a54">Output</a>) * (container-&gt;<a class="code" href="struct_data_container.html#e52a283505cf5b64b4c50c462609fa24">DataDesiredOutput</a>[c][j] - neuron-&gt;<a class="code" href="class_neuron.html#29c2c02a361c9d7028472e5d92cd4a54">Output</a>) ;
<a name="l00093"></a>00093                                         }
<a name="l00094"></a>00094                                         <span class="keywordflow">else</span>
<a name="l00095"></a>00095                                         {
<a name="l00096"></a>00096                                                 <span class="comment">//Fehler berechnen = error = f_act'(net) * Sum( Fehler vorherigen Layers * Gewicht)</span>
<a name="l00097"></a>00097                                                 error= ActivationFunctionDerivate(net-&gt;<a class="code" href="class_neural_network.html#929f1f7a270c41cfd9e987b88e048aaf">FunctionType</a>,neuron-&gt;<a class="code" href="class_neuron.html#29c2c02a361c9d7028472e5d92cd4a54">Output</a>) * sumDeltaErrWeights[j];
<a name="l00098"></a>00098 
<a name="l00099"></a>00099                                                 sumDeltaErrWeights[j] = 0; <span class="comment">// zur√ºcksetzen um es f√ºr nachfolgenden Layer zu berechnen</span>
<a name="l00100"></a>00100                                         }
<a name="l00101"></a>00101 
<a name="l00102"></a>00102                                         <span class="comment">//Fehler mit lernrate multipliziert</span>
<a name="l00103"></a>00103                                         error_mul_alpha = alpha * error;
<a name="l00104"></a>00104 
<a name="l00105"></a>00105 
<a name="l00106"></a>00106                                         <span class="keywordflow">if</span>(batchSize &lt; 2)
<a name="l00107"></a>00107                                         {
<a name="l00108"></a>00108                                                 <span class="keywordflow">for</span>(k=0;k&lt;inputVectorCount;++k)
<a name="l00109"></a>00109                                                 {
<a name="l00110"></a>00110                                                         <span class="comment">//Fehler f√ºr die Neuronen des n√§chsten Layers l-1 berechnen</span>
<a name="l00111"></a>00111                                                         sumDeltaErrWeightsNext[k] += weights[k] * error;
<a name="l00112"></a>00112 
<a name="l00113"></a>00113 
<a name="l00114"></a>00114                                                         <span class="comment">//delta gewicht berechnen =</span>
<a name="l00115"></a>00115                                                         lastDeltaWeights[k]=error_mul_alpha * inputVector[k] + lastDeltaWeights[k] * momentum;
<a name="l00116"></a>00116 
<a name="l00117"></a>00117                                                         <span class="comment">//Gewicht anpassen</span>
<a name="l00118"></a>00118                                                         weights[k]+=lastDeltaWeights[k] ;
<a name="l00119"></a>00119                                                         <span class="comment">//letzte gewicht√§nderung speichern f√ºr momentum</span>
<a name="l00120"></a>00120                                                         <span class="comment">//LastDeltaWeights[k]=deltaWeight;</span>
<a name="l00121"></a>00121 
<a name="l00122"></a>00122 
<a name="l00123"></a>00123                                                 }
<a name="l00124"></a>00124 
<a name="l00125"></a>00125                                                 <span class="comment">//Bias einzelnd berechnen</span>
<a name="l00126"></a>00126                                                 deltaWeight=  error_mul_alpha  + lastDeltaWeights[inputVectorCount] * momentum;
<a name="l00127"></a>00127                                                 weights[inputVectorCount]+=deltaWeight;
<a name="l00128"></a>00128                                                 lastDeltaWeights[inputVectorCount]=deltaWeight;
<a name="l00129"></a>00129                                         }
<a name="l00130"></a>00130                                         <span class="keywordflow">else</span>
<a name="l00131"></a>00131                                         {
<a name="l00132"></a>00132                                                 <span class="keywordflow">if</span>((c+1) % batchSize ==0 <span class="comment">/*&amp;&amp; c != 0*/</span>)
<a name="l00133"></a>00133                                                 {
<a name="l00134"></a>00134                                                         <span class="keywordflow">for</span>(k=0;k&lt;layer-&gt;<a class="code" href="class_layer.html#74697d5bce4cac5ae28f7d2aa266d9d8">InputVectorCount</a>;++k)
<a name="l00135"></a>00135                                                         {
<a name="l00136"></a>00136                                                                 <span class="comment">//Fehler f√ºr die Neuronen des n√§chsten Layers l-1 berechnen</span>
<a name="l00137"></a>00137                                                                 sumDeltaErrWeightsNext[k] += weights[k] * error;
<a name="l00138"></a>00138 
<a name="l00139"></a>00139 
<a name="l00140"></a>00140                                                                 <span class="comment">//Gewichte anpassen</span>
<a name="l00141"></a>00141                                                                 deltaWeights[k]+=  error_mul_alpha * inputVector[k];
<a name="l00142"></a>00142 
<a name="l00143"></a>00143                                                                 deltaWeights[k]+=deltaWeights[k]+ lastDeltaWeights[k] * momentum;
<a name="l00144"></a>00144                                                                 weights[k]+=deltaWeights[k] ;
<a name="l00145"></a>00145                                                                 lastDeltaWeights[k]=deltaWeights[k];
<a name="l00146"></a>00146                                                                 deltaWeights[k]=0;
<a name="l00147"></a>00147 
<a name="l00148"></a>00148 
<a name="l00149"></a>00149 
<a name="l00150"></a>00150                                                         }
<a name="l00151"></a>00151 
<a name="l00152"></a>00152                                                         <span class="comment">//Bias einzelnd berechnen</span>
<a name="l00153"></a>00153                                                         deltaWeights[inputVectorCount] +=  error_mul_alpha * neuron-&gt;<a class="code" href="class_neuron.html#4ad6b9e6418fd2d739906bd47e887ae3">Bias</a> ;
<a name="l00154"></a>00154 
<a name="l00155"></a>00155                                                         deltaWeights[inputVectorCount]+= lastDeltaWeights[inputVectorCount] * momentum;
<a name="l00156"></a>00156                                                         weights[inputVectorCount]+=deltaWeights[inputVectorCount];
<a name="l00157"></a>00157                                                         lastDeltaWeights[inputVectorCount]=deltaWeights[inputVectorCount];
<a name="l00158"></a>00158                                                         deltaWeights[inputVectorCount] =0;
<a name="l00159"></a>00159                                                 }
<a name="l00160"></a>00160                                                 <span class="keywordflow">else</span>
<a name="l00161"></a>00161                                                 {
<a name="l00162"></a>00162                                                         <span class="keywordflow">for</span>(k=0;k&lt;inputVectorCount;++k)
<a name="l00163"></a>00163                                                         {
<a name="l00164"></a>00164 
<a name="l00165"></a>00165                                                                 sumDeltaErrWeightsNext[k] += weights[k] * error;
<a name="l00166"></a>00166                                                                 <span class="comment">//Gewichte anpassen</span>
<a name="l00167"></a>00167                                                                 deltaWeights[k]+=  error_mul_alpha * inputVector[k];
<a name="l00168"></a>00168 
<a name="l00169"></a>00169                                                         }
<a name="l00170"></a>00170                                                         <span class="comment">//Bias einzelnd berechnen</span>
<a name="l00171"></a>00171                                                         deltaWeights[inputVectorCount] +=  error_mul_alpha * neuron-&gt;<a class="code" href="class_neuron.html#4ad6b9e6418fd2d739906bd47e887ae3">Bias</a> ;
<a name="l00172"></a>00172                                                 }
<a name="l00173"></a>00173                                         }
<a name="l00174"></a>00174                                 }
<a name="l00175"></a>00175                                 sumDeltaErrWeights = layer-&gt;SumDeltaErrWeights;
<a name="l00176"></a>00176                         }
<a name="l00177"></a>00177 
<a name="l00178"></a>00178                         <span class="comment">/*Letzten Layer einzelnd berechnen da hier einige Abfragen wegfallen</span>
<a name="l00179"></a>00179 <span class="comment">                        da keine berechnungen der Fehler f√ºr den n√§chsten Layer n√∂tig sind*/</span>
<a name="l00180"></a>00180 
<a name="l00181"></a>00181                         layer = &amp;net-&gt;Layers[0];
<a name="l00182"></a>00182                         inputVector = layer-&gt;InputVector;
<a name="l00183"></a>00183                         inputVectorCount=layer-&gt;InputVectorCount;
<a name="l00184"></a>00184                         <span class="comment">//sumDeltaErrWeights = layer-&gt;SumDeltaErrWeights;</span>
<a name="l00185"></a>00185 
<a name="l00186"></a>00186                         <span class="keywordflow">for</span>(j=0;j&lt;layer-&gt;NeuronCount;++j)
<a name="l00187"></a>00187                         {
<a name="l00188"></a>00188                                 neuron = &amp;layer-&gt;Neurons[j];
<a name="l00189"></a>00189 
<a name="l00190"></a>00190 
<a name="l00191"></a>00191 
<a name="l00192"></a>00192                                 error= ActivationFunctionDerivate(net-&gt;FunctionType,neuron-&gt;Output) * sumDeltaErrWeights[j];
<a name="l00193"></a>00193                                 sumDeltaErrWeights[j] = 0;
<a name="l00194"></a>00194 
<a name="l00195"></a>00195                                 error_mul_alpha = alpha * error;
<a name="l00196"></a>00196 
<a name="l00197"></a>00197                                 weights = neuron-&gt;Weights;
<a name="l00198"></a>00198                                 lastDeltaWeights = neuron-&gt;LastDeltaWeights;
<a name="l00199"></a>00199                                 deltaWeights = neuron-&gt;DeltaWeights;
<a name="l00200"></a>00200 
<a name="l00201"></a>00201                                 <span class="keywordflow">if</span>(batchSize &lt; 2)
<a name="l00202"></a>00202                                 {
<a name="l00203"></a>00203 
<a name="l00204"></a>00204                                         <span class="keywordflow">for</span>(k=0;k!=inputVectorCount;++k)
<a name="l00205"></a>00205                                         {
<a name="l00206"></a>00206                                                 lastDeltaWeights[k]= (lastDeltaWeights[k] * momentum) + (error_mul_alpha * inputVector[k]);
<a name="l00207"></a>00207                                                 weights[k]+=lastDeltaWeights[k] ;
<a name="l00208"></a>00208                                                 <span class="comment">//LastDeltaWeights[k]=deltaWeight;</span>
<a name="l00209"></a>00209                                         }
<a name="l00210"></a>00210 
<a name="l00211"></a>00211                                         <span class="comment">//Bias</span>
<a name="l00212"></a>00212                                         deltaWeight=  error_mul_alpha  + lastDeltaWeights[inputVectorCount] * momentum;
<a name="l00213"></a>00213                                         weights[inputVectorCount]+=deltaWeight;
<a name="l00214"></a>00214                                         lastDeltaWeights[inputVectorCount]=deltaWeight;
<a name="l00215"></a>00215                                 }
<a name="l00216"></a>00216                                 <span class="keywordflow">else</span>
<a name="l00217"></a>00217                                 {
<a name="l00218"></a>00218 
<a name="l00219"></a>00219                                         <span class="keywordflow">if</span>((c+1) % batchSize ==0)
<a name="l00220"></a>00220                                         {
<a name="l00221"></a>00221                                                 <span class="keywordflow">for</span>(k=0;k&lt;inputVectorCount;++k)
<a name="l00222"></a>00222                                                 {
<a name="l00223"></a>00223                                                         <span class="comment">//Gewichte anpassen</span>
<a name="l00224"></a>00224                                                         deltaWeights[k]+=  error_mul_alpha * inputVector[k];
<a name="l00225"></a>00225 
<a name="l00226"></a>00226                                                         deltaWeights[k]+=deltaWeights[k]+ lastDeltaWeights[k] * momentum;
<a name="l00227"></a>00227                                                         weights[k]+=deltaWeights[k] ;
<a name="l00228"></a>00228                                                         lastDeltaWeights[k]=deltaWeights[k];
<a name="l00229"></a>00229                                                         deltaWeights[k]=0;
<a name="l00230"></a>00230                                                 }
<a name="l00231"></a>00231 
<a name="l00232"></a>00232                                                 <span class="comment">//Bias einzelnd berechnen</span>
<a name="l00233"></a>00233                                                 deltaWeights[inputVectorCount] +=  error_mul_alpha * neuron-&gt;Bias ;
<a name="l00234"></a>00234 
<a name="l00235"></a>00235                                                 deltaWeights[inputVectorCount]+= lastDeltaWeights[inputVectorCount] * momentum;
<a name="l00236"></a>00236                                                 weights[inputVectorCount]+=deltaWeights[inputVectorCount];
<a name="l00237"></a>00237                                                 lastDeltaWeights[inputVectorCount]=deltaWeights[inputVectorCount];
<a name="l00238"></a>00238                                                 deltaWeights[inputVectorCount] =0;
<a name="l00239"></a>00239                                         }
<a name="l00240"></a>00240                                         <span class="keywordflow">else</span>
<a name="l00241"></a>00241                                         {
<a name="l00242"></a>00242                                                 <span class="keywordflow">for</span>(k=0;k&lt;inputVectorCount;++k)
<a name="l00243"></a>00243                                                 {
<a name="l00244"></a>00244                                                         deltaWeights[k]+=  error_mul_alpha * inputVector[k];
<a name="l00245"></a>00245                                                 }
<a name="l00246"></a>00246                                                 <span class="comment">//Bias einzelnd berechnen</span>
<a name="l00247"></a>00247                                                 deltaWeights[inputVectorCount] +=  error_mul_alpha * neuron-&gt;Bias ;
<a name="l00248"></a>00248                                         }
<a name="l00249"></a>00249                                 }
<a name="l00250"></a>00250                         }
<a name="l00251"></a>00251                 }
<a name="l00252"></a>00252 
<a name="l00253"></a>00253                 net-&gt;MeanSquareError /= container-&gt;DataCount;
<a name="l00254"></a>00254                 <span class="comment">//Result[l] = net-&gt;MeanSquareError;</span>
<a name="l00255"></a>00255 
<a name="l00256"></a>00256                 measureResult[l].MeanSquareError = net-&gt;MeanSquareError;
<a name="l00257"></a>00257                 <span class="comment">//unsigned long long asdasd = GetTimeMs64() - time64;</span>
<a name="l00258"></a>00258                 measureResult[l].ExecuteTime =GetTimeMs64() - time64;<span class="comment">// GetTimeMs64() - time64;</span>
<a name="l00259"></a>00259 
<a name="l00260"></a>00260                 <span class="keywordflow">if</span>(net-&gt;MeanSquareError &lt;= threshold)
<a name="l00261"></a>00261                         <span class="keywordflow">break</span>;
<a name="l00262"></a>00262 
<a name="l00263"></a>00263                 <span class="keywordflow">if</span>(l == loopcount-1)
<a name="l00264"></a>00264                         std::cout &lt;&lt; <span class="stringliteral">"ERROR: "</span> &lt;&lt; net-&gt;MeanSquareError &lt;&lt; <span class="stringliteral">"\n"</span>;
<a name="l00265"></a>00265         }
<a name="l00266"></a>00266 
<a name="l00267"></a>00267         net-&gt;CopyNeuralNetworkMeasure(measureResult,l);
<a name="l00268"></a>00268         <span class="comment">/*net-&gt;MSEIterationResultsLenght = l;</span>
<a name="l00269"></a>00269 <span class="comment"></span>
<a name="l00270"></a>00270 <span class="comment">        net-&gt;MSEIterationResults = Result;*/</span>
<a name="l00271"></a>00271         <span class="keywordflow">if</span>(measureResult)
<a name="l00272"></a>00272                 <span class="keyword">delete</span> [] measureResult;
<a name="l00273"></a>00273 
<a name="l00274"></a>00274         <span class="comment">//return 0;</span>
<a name="l00275"></a>00275         <span class="comment">//return MeanSquareError;</span>
<a name="l00276"></a>00276 }
<a name="l00277"></a>00277 
</pre></div><hr size="1"><address style="align: right;"><small>Erzeugt am Tue Aug 12 00:17:27 2014 f¸r NeuralNetworkTrainer von&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.4.5 </small></address>
</body>
</html>
