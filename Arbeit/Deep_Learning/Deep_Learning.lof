\contentsline {figure}{\numberline {1}{\ignorespaces Neuron (Quelle: http://commons.wikimedia.org/wiki/File:Neuron\_-\_annotated.svg) \relax }}{3}
\contentsline {figure}{\numberline {2}{\ignorespaces Zwei miteinander verbundene Neuronen in einem Neuronalen Netz\relax }}{4}
\contentsline {figure}{\numberline {3}{\ignorespaces Neuronales Netz. Die Ein- und Ausgabeebene verwendet eine lineare Aktivierungsfunktion, die versteckte Ebene den Tangens Hyperbolicus.\relax }}{7}
\contentsline {figure}{\numberline {4}{\ignorespaces \IeC {\"U}bergangsgraph. Knoten geben m\IeC {\"o}gliche Zust\IeC {\"a}nde an, Kanten sind Wahrscheinlichkeiten in einen Zustand zu wechseln\relax }}{10}
\contentsline {figure}{\numberline {5}{\ignorespaces Modell eines Hopfield Netzes. Alle Neuronen besitzen eine Ein- und Ausgabe und sind \IeC {\"u}ber Gewichte jeweils mit allen anderen Neuronen verbunden\relax }}{10}
\contentsline {figure}{\numberline {6}{\ignorespaces Modell einer Boltzmann Maschine. Gelbe Kreise stellen verstecke Neuronen dar, wei\IeC {\ss }e Kreise sind sichtbare Neuronen.\relax }}{11}
\contentsline {figure}{\numberline {7}{\ignorespaces Modell einer eingeschr\IeC {\"a}nkten Boltzmann Maschine. Oben sind die versteckten Knoten und unten die nach au\IeC {\ss }en sichtbaren Knoten.\relax }}{13}
\contentsline {figure}{\numberline {8}{\ignorespaces Markov-Kette mit Gibbs Sampling. Diese wird mit einem Trainingsvektor initialisiert. Die sichtbaren und unsichtbaren Neuronen werden abwechselnd neu berechnet bis ein Equilibriumszustand erreicht wird.\relax }}{16}
\contentsline {figure}{\numberline {9}{\ignorespaces Hyprid Netzwerk. Die Ebenen $H_3$ und $H_2$ sind mit ungerichteten Kanten verbunden und bilden einen Assoziativspeicher. Die anderen Ebenen sind mit gerichteten Kanten verbunden.\relax }}{18}
\contentsline {figure}{\numberline {10}{\ignorespaces Beispiel f\IeC {\"u}r den Explained Away Effekt. Der Bias von $-10$ am Erdbeben-knoten bedeutet dass dieser ohne beobachtet zu werden $e^{10}$ mal wahrscheinlicher aus als aktiviert ist. Beide Ereignisse zusammen haben eine Wahrscheinlichkeit von $e^{-20}$, da diese unabh\IeC {\"a}ngig voneinander sind. Dies ist also sehr unwahrscheinlich. Deshalb erkl\IeC {\"a}rt die Aktivierung eines Knotens den anderen weg.\relax }}{19}
\contentsline {figure}{\numberline {11}{\ignorespaces Netzwerk mit Softmax-Gruppe zum Klassifizieren. Aus Sicht von $H_3$ werden die Softmax-Neuronen als normale Eingabe betrachtet. An der Softmax-Gruppe kann sp\IeC {\"a}ter die gefundene Klasse abgelesen werden.\relax }}{20}
\contentsline {figure}{\numberline {12}{\ignorespaces Netzwerk mit Ausgabeschicht zur Klassifizierung. Die Gewichte zwischen $H_3$ und der Ausgabe werden mit Backpropagation trainiert.\relax }}{21}
\contentsline {figure}{\numberline {13}{\ignorespaces Klassenherarchie des verwendeten Frameworks, rot umrandete Klassen wurden in dieser Arbeit hinzugef\IeC {\"u}gt.\relax }}{22}
